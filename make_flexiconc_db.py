# -*- coding: utf-8 -*-
"""make_osw_flexiconc_db.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UBblxdHgkP5hgScImgqelN9wlRCVrAO0
"""

#!/usr/bin/env python3
"""
Build FlexiConc DB from OSW export and patch spans_file with metadata.

Steps:
1) Load /export/pl and /export/en into FlexiConc via TextImport.
2) Find the created SQLite database file.
3) Ensure spans_file has extra columns and patch from file_metadata.csv.

Usage:
  python make_osw_flexiconc_db.py --export-root /path/to/exports --db-name osw.sqlite
"""

import argparse
import sqlite3
from pathlib import Path
from typing import Optional, List
import time
import pandas as pd
import json
from datetime import datetime

# FlexiConc
from flexiconc import TextImport

#ENLENS pdf_extract
from helper import extract_text_from_pdf_robust, preprocess_pdf_text

def _normalize_dots(s: str) -> str:
    # collapse multiple dots like ".." -> "."
    while ".." in s:
        s = s.replace("..", ".")
    return s

def extract_doi_or_slug_key(fname: str) -> str:
    """
    Build a stable deduplication key from a filename:
    - Prefer the substring after 'doi.org.' or 'www.iea.org.'
    - Lowercase, strip .pdf/.PDF, collapse repeated dots
    - Remove repeated 'doi.org.' prefixes if present multiple times
    """
    f = fname.strip().lower()
    if f.endswith(".pdf"):
        f = f[:-4]

    # where does the DOI / slug start?
    key_region = None
    for anchor in ["doi.org.", "www.iea.org."]:
        if anchor in f:
            key_region = f.split(anchor, 1)[1]
            # strip any additional leading 'doi.org.'s (seen in some filenames)
            while key_region.startswith("doi.org."):
                key_region = key_region[len("doi.org."):]
            break

    # fallback: if no anchor found, just use the stem (won't dedup across different prefixes)
    if key_region is None:
        key_region = Path(fname).stem.lower()

    key_region = _normalize_dots(key_region)
    return key_region

def build_filtered_pdf_dict(pdf_dir: Path, filenames: list[str]) -> dict[str, str]:
    """
    Return {basename_without_ext: full_path} keeping the FIRST file seen
    for each unique DOI/slug key.
    """
    seen = set()
    filtered = {}
    for fname in filenames:
        key = extract_doi_or_slug_key(fname)
        if key in seen:
            continue
        seen.add(key)
        stem = Path(fname).stem
        filtered[stem] = str((pdf_dir / fname).resolve())
    return filtered


def load_into_flexiconc(export_root: Path, db_name: str) -> Path:
    """
    Load PL/EN docs into FlexiConc.
    Try to get a DB path; if the API doesn't expose one, we search heuristically.
    """
    pl_dir = export_root / "pl"
    en_dir = export_root / "en"
    if not pl_dir.exists() and not en_dir.exists():
        raise SystemExit(f"No 'pl' or 'en' directories under: {export_root}")

    ti = TextImport()
    paths: List[str] = []
    if pl_dir.exists():
        paths.append(str(pl_dir))
    if en_dir.exists():
        paths.append(str(en_dir))

    print(f"üì• Loading {len(paths)} folders into FlexiConc...")
    ti.load_files(paths=paths, use_spacy=False)

    # Try to discover DB path automatically
    # Strategy A: attribute
    db_path: Optional[Path] = None
    cand_attrs = ("db_path", "database_path", "dbfile", "db_file")
    for attr in cand_attrs:
        if hasattr(ti, attr):
            v = getattr(ti, attr)
            if v:
                db_path = Path(v)
                break

    # Strategy B: search for recent .sqlite files under export_root (and its parent)
    def newest_sqlite(base: Path) -> Optional[Path]:
        sqlites = []
        try:
            sqlites.extend(base.rglob("*.sqlite"))
            sqlites.extend(base.rglob("*.db"))
        except OSError as e:
            print(f"‚ö†Ô∏è Skipping {base}: {e}")
            return None
        if not sqlites:
            return None
        sqlites.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return sqlites[0]


    if db_path is None:
        # give the system a moment to flush files
        time.sleep(0.5)
        db_path = newest_sqlite(export_root)
    

    if db_path is None:
        print("‚ö†Ô∏è  Could not auto-detect a DB file created by TextImport.")
        # As a fallback, create an empty DB name under export_root and let the user patch manually
        db_path = export_root / db_name
        print(f"Creating a new DB stub at: {db_path}")
        sqlite3.connect(str(db_path)).close()

    print(f"‚úÖ Using database: {db_path}")
    return db_path


def ensure_columns(conn: sqlite3.Connection, table: str, columns: dict):
    """Ensure columns exist on table; columns is {name: SQLTYPE}."""
    cur = conn.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    existing = {row[1] for row in cur.fetchall()}
    for col, typ in columns.items():
        if col not in existing:
            cur.execute(f"ALTER TABLE {table} ADD COLUMN {col} {typ}")
    conn.commit()


def detect_filename_column(conn: sqlite3.Connection, table: str) -> Optional[str]:
    """Guess which column stores filename/path in spans_file."""
    cur = conn.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    cols = [r[1] for r in cur.fetchall()]
    for candidate in ("filename", "file", "path", "name"):
        if candidate in cols:
            return candidate
    return None


def patch_spans_file_with_metadata(db_path, meta_csv_path):
    import sqlite3, pandas as pd
    from pathlib import Path

    db_path = Path(db_path)
    meta_csv_path = Path(meta_csv_path)

    if not db_path.exists():
        print(f"‚ö†Ô∏è  DB not found at {db_path}. Skipping metadata patch.")
        return
    if not meta_csv_path.exists():
        print(f"‚ö†Ô∏è  file_metadata.csv not found at {meta_csv_path}. Skipping metadata patch.")
        return

    conn = sqlite3.connect(str(db_path))
    cur  = conn.cursor()

    # --- discover tables ---
    cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
    tables = {r[0] for r in cur.fetchall()}

    # Prefer span-level, else fall back to file-level
    span_candidates = ["spans_file", "spans", "segments", "tokens"]
    file_candidates = ["files", "documents", "texts"]

    target_table = next((t for t in span_candidates if t in tables), None)
    level = "span"
    if not target_table:
        target_table = next((t for t in file_candidates if t in tables), None)
        level = "file"

    if not target_table:
        print(f"‚ö†Ô∏è  No suitable table found. Available tables: {sorted(tables)}. Skipping patch.")
        conn.close()
        return

    # --- pick a filename column ---
    cur.execute(f"PRAGMA table_info('{target_table}')")
    cols = [r[1] for r in cur.fetchall()]
    filename_candidates = ["filepath","path","filename","file","name"]
    filename_col = next((c for c in filename_candidates if c in cols), None)

    if not filename_col:
        print(f"‚ö†Ô∏è  No filename-like column in {target_table}. Columns: {cols}. Skipping patch.")
        conn.close()
        return

    # --- ensure metadata columns exist ---
    want_cols = {
        "text_id": "TEXT",
        "lang": "TEXT",
        "bibl_id": "TEXT",
        "title_a": "TEXT",
        "title_j": "TEXT",
        "author": "TEXT",
        "date_published": "TEXT",
        "url": "TEXT",
    }
    existing = set(cols)
    for col, typ in want_cols.items():
        if col not in existing:
            try:
                cur.execute(f"ALTER TABLE {target_table} ADD COLUMN {col} {typ}")
            except sqlite3.OperationalError as e:
                print(f"‚ö†Ô∏è  Could not add column {col} to {target_table}: {e}")

    # --- load metadata and update ---
    meta = pd.read_csv(meta_csv_path)
    if "filepath" not in meta.columns:
        print(f"‚ö†Ô∏è  file_metadata.csv missing 'filepath' column. Skipping patch.")
        conn.close()
        return

    def _norm(s): return str(s).strip().replace("\\", "/")
    meta["__key"] = meta["filepath"].map(_norm)

    cur.execute(f"SELECT rowid, {filename_col} FROM {target_table}")
    rows = cur.fetchall()

    updated = 0
    for rowid, fn in rows:
        key = _norm(fn)
        hit = meta.loc[meta["__key"] == key]
        if not hit.empty:
            rec = hit.iloc[0]
            params = (
                rec.get("text_id", ""),
                rec.get("lang", ""),
                rec.get("bibl_id", ""),
                rec.get("title_a", ""),
                rec.get("title_j", ""),
                rec.get("author", ""),
                rec.get("date_published", ""),
                rec.get("url", ""),
                rowid,
            )
            cur.execute(
                f"""UPDATE {target_table}
                    SET text_id=?, lang=?, bibl_id=?, title_a=?, title_j=?, author=?, date_published=?, url=?
                  WHERE rowid=?""",
                params
            )
            updated += 1

    conn.commit()
    conn.close()
    print(f"üîó Metadata patch ({level}-level): updated {updated} rows in '{target_table}' via '{filename_col}'.")

# --- ADD below existing functions: helper writers for export material ---

def _ensure_export_dirs(export_root: Path, lang: str) -> Path:
    out_dir = export_root / lang
    out_dir.mkdir(parents=True, exist_ok=True)
    return out_dir

def _append_metadata_row(meta_csv: Path, row: dict):
    # Create or append to file_metadata.csv with a stable column order
    cols = ["filepath","text_id","lang","bibl_id","title_a","title_j","author","date_published","url"]
    df_row = {c: row.get(c, "") for c in cols}
    if meta_csv.exists():
        import pandas as pd
        df = pd.read_csv(meta_csv)
        # append without losing column order
        df = pd.concat([df, pd.DataFrame([df_row])], ignore_index=True)
        df.to_csv(meta_csv, index=False)
    else:
        import pandas as pd
        pd.DataFrame([df_row]).to_csv(meta_csv, index=False)

def write_text_from_pdf(export_root: Path, pdf_path: Path, lang: str = "pl") -> Path:
    """
    Extract + preprocess PDF, save to export_root/lang/<stem>.txt,
    and append a metadata row.
    """
    out_dir = _ensure_export_dirs(export_root, lang)
    raw = extract_text_from_pdf_robust(str(pdf_path))
    processed = preprocess_pdf_text(raw)
    out_txt = out_dir / (pdf_path.stem + ".txt")
    out_txt.write_text(processed, encoding="utf-8")

    meta_csv = export_root / "file_metadata.csv"
    _append_metadata_row(meta_csv, {
        "filepath": str(out_txt),
        "text_id": pdf_path.stem,
        "lang": lang,
        "bibl_id": "",
        "title_a": pdf_path.stem,
        "title_j": "",
        "author": "",
        "date_published": datetime.now().date().isoformat(),
        "url": "",
    })
    return out_txt

def write_text_from_sentences(export_root: Path, sentences: list, base_name: str, lang: str = "pl") -> Path:
    """
    Save a sentence-per-line file from your `result['production_output']['sentence_analyses']`.
    """
    out_dir = _ensure_export_dirs(export_root, lang)
    lines = []
    for s in sentences:
        # Robust to either dicts with 'sentence_text' or plain strings
        if isinstance(s, dict) and "sentence_text" in s:
            lines.append(s["sentence_text"])
        else:
            lines.append(str(s))
    text = "\n".join(lines).strip()
    out_txt = out_dir / f"{base_name}.txt"
    out_txt.write_text(text, encoding="utf-8")

    meta_csv = export_root / "file_metadata.csv"
    _append_metadata_row(meta_csv, {
        "filepath": str(out_txt),
        "text_id": base_name,
        "lang": lang,
        "bibl_id": "",
        "title_a": base_name,
        "title_j": "",
        "author": "",
        "date_published": datetime.now().date().isoformat(),
        "url": "",
    })
    return out_txt


# --- main(): LI options and preprocessing ---
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--export-root", required=True, help="Directory produced by pelcra_parser.py OR the place to stage new texts")
    ap.add_argument("--db-name", default="osw.sqlite", help="Optional DB filename (used if auto-detection fails)")
    ap.add_argument("--pdf", help="Path to a single PDF to ingest via helper.extract_text_from_pdf_robust + preprocess_pdf_text")
    ap.add_argument("--pdf-list", help="Path to a text file with PDF filenames (one per line)")
    ap.add_argument("--pdf-list-json", help="Path to a JSON file containing a list of PDF filenames")
    ap.add_argument("--pdf-dir", default="/content/drive/MyDrive/ENLENS/PDF_paper",
                help="Directory where the PDFs live (used with --pdf-list/--pdf-list-json)")
    ap.add_argument("--db-path", help="Exact destination for the SQLite DB (will copy via VACUUM INTO)")
    ap.add_argument("--dedup", action="store_true",
                help="If set, deduplicate filenames by DOI/slug before processing")
    ap.add_argument("--sentences-json", help="Path to a JSON file containing `result` with result['production_output']['sentence_analyses']")
    ap.add_argument("--base-name", default="from_sentences", help="Basename for the text file when using --sentences-json")
    ap.add_argument("--lang", default="pl", choices=["pl","en"], help="Language bucket for FlexiConc import")

    args = ap.parse_args()

    export_root = Path(args.export_root)
    meta_csv = export_root / "file_metadata.csv"
    export_root.mkdir(parents=True, exist_ok=True)

    # If user gave a PDF, convert & stage it
    if args.pdf:
        pdf_path = Path(args.pdf)
        if not pdf_path.exists():
            raise SystemExit(f"PDF not found: {pdf_path}")
        print(f"üßæ Preparing text from PDF: {pdf_path}")
        write_text_from_pdf(export_root, pdf_path, lang=args.lang)

    # If user gave a result json with sentence_analyses, convert & stage it
    if args.sentences_json:
        p = Path(args.sentences_json)
        if not p.exists():
            raise SystemExit(f"JSON not found: {p}")
        print(f"üßæ Preparing text from sentence analyses: {p}")
        data = json.loads(p.read_text(encoding="utf-8"))
        # Expect: data['production_output']['sentence_analyses'] OR directly a list
        if isinstance(data, dict):
            prod = data.get("production_output", {})
            sentences = prod.get("sentence_analyses", data.get("sentences", []))
        else:
            sentences = data
        if not sentences:
            raise SystemExit("No sentences found under ['production_output']['sentence_analyses'] or ['sentences'].")

        write_text_from_sentences(export_root, sentences, base_name=args.base_name, lang=args.lang)
        
    
    if args.pdf_list or args.pdf_list_json:
        import json

        pdf_dir = Path(args.pdf_dir)
        if not pdf_dir.exists():
            raise SystemExit(f"PDF directory not found: {pdf_dir}")

        # Load filename list
        if args.pdf_list_json:
            names = json.loads(Path(args.pdf_list_json).read_text(encoding="utf-8"))
            if not isinstance(names, list):
                raise SystemExit("--pdf-list-json must contain a JSON array of filenames")
        else:
            # --pdf-list is a text file: one filename per line
            names = [ln.strip() for ln in Path(args.pdf_list).read_text(encoding="utf-8").splitlines() if ln.strip()]

        if args.dedup:
            PDF_filtered = build_filtered_pdf_dict(pdf_dir, names)
            print(f"‚úÖ Deduplicated by DOI/slug: kept {len(PDF_filtered)} / {len(names)} files")
        else:
            # no dedup: keep all
            PDF_filtered = {Path(n).stem: str((pdf_dir / n).resolve()) for n in names}
            print(f"‚ÑπÔ∏è Dedup disabled: keeping all {len(PDF_filtered)} files")

        # Process each filtered PDF via your helper path
        for base_name, pdf_path in PDF_filtered.items():
            print(f"üßæ Preparing text from PDF: {pdf_path}")
            write_text_from_pdf(Path(args.export_root), Path(pdf_path), lang=args.lang)

    # 1) Load into FlexiConc and find DB (unchanged)
    db_path = load_into_flexiconc(export_root, args.db_name)
    
    # --- 1) Import and get a DB path if the importer returns one ---
    db_path = load_into_flexiconc(export_root, args.db_name)  # should return a path or None

    # --- 2) If the importer didn't tell us, or the file doesn't exist, discover under export_root ---


    def discover_sqlite_under(root):
        root = Path(root)
        hits = list(root.rglob("*.sqlite"))
        return sorted(hits, key=lambda p: p.stat().st_mtime, reverse=True)

    if not db_path or not Path(db_path).exists():
        candidates = discover_sqlite_under(args.export_root)
        if candidates:
            db_path = str(candidates[0])
            print(f"‚úÖ Using database discovered under export_root: {db_path}")
        else:
            raise SystemExit("No .sqlite found under export_root; cannot patch.")

    # --- 3) (Optional) Force a specific location if user provided --db-path ---
    # Add to argparse if you haven't:
    # ap.add_argument("--db-path", help="Exact destination for the SQLite DB (will copy via VACUUM INTO)")
    if getattr(args, "db_path", None):
        dest = Path(args.db_path)
        dest.parent.mkdir(parents=True, exist_ok=True)
        if str(dest) != str(db_path):
            # Use SQLite VACUUM INTO to copy safely
            import sqlite3
            print(f"ü™Ñ Copying DB to requested path via VACUUM INTO: {dest}")
            src_conn = sqlite3.connect(db_path)
            try:
                src_conn.execute(f"VACUUM INTO '{dest.as_posix()}'")
            finally:
                src_conn.close()
            db_path = str(dest)

    # --- 4) Patch metadata (tolerant version that auto-detects the right table) ---
    patch_spans_file_with_metadata(db_path, meta_csv)

    
    print("\nüéâ Done. You can now query with FlexiConc and see file-level metadata on results.")
if __name__ == "__main__":
    main()