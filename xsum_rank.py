#!/usr/bin/env python3
import os
import sys
import ast
import argparse
import subprocess
from argparse import Namespace

import torch
from transformers import BertTokenizer

def clean(text):
    """Basic text cleaning"""
    return text.strip()

def prepare_data_for_presum(sentences, tokenizer, max_len=512):
    """
    Prepare data in the format expected by PreSumm model.

    PreSumm expects:
    - src: input token ids with [CLS] at the beginning of each sentence
    - segs: segment ids (alternating 0,1,0,1... for sentences)
    - clss: positions of [CLS] tokens
    - mask_src: source attention mask
    - mask_cls: mask for [CLS] tokens
    """

    # Step 1: Build input with [CLS] tokens
    input_ids = [tokenizer.cls_token_id]  # Start with [CLS]
    segment_ids = [0]  # Start with segment ID for first [CLS]
    clss_positions = [0]  # First [CLS] is at position 0

    for i, sentence in enumerate(sentences):
        # Clean and tokenize sentence (without [CLS] and [SEP])
        sent_tokens = tokenizer.encode(clean(sentence), add_special_tokens=False)

        # Alternate segment ids (0, 1, 0, 1, ...)
        seg_id = i % 2

        # Add tokens for this sentence
        input_ids.extend(sent_tokens)
        segment_ids.extend([seg_id] * len(sent_tokens))

        # Add [CLS] for next sentence (except for last sentence)
        if i < len(sentences) - 1:
            next_cls_pos = len(input_ids)
            input_ids.append(tokenizer.cls_token_id)
            # [CLS] token gets the same segment ID as the sentence it follows
            segment_ids.append(seg_id)
            clss_positions.append(next_cls_pos)

    # Step 2: Truncate if too long
    if len(input_ids) > max_len:
        input_ids = input_ids[:max_len]
        segment_ids = segment_ids[:max_len]
        # Keep only [CLS] positions that are within the truncated length
        clss_positions = [pos for pos in clss_positions if pos < max_len]

    # Step 3: Create masks
    src_len = len(input_ids)
    clss_len = len(clss_positions)

    # Source mask (1 for real tokens, 0 for padding)
    mask_src = [1] * src_len

    # CLS mask (1 for real [CLS] tokens, 0 for padding)
    mask_cls = [1] * clss_len

    return {
        'src': input_ids,
        'segs': segment_ids,
        'clss': clss_positions,
        'mask_src': mask_src,
        'mask_cls': mask_cls
    }

def batch_data(data_list, pad_token_id=0):
    """Convert list of data dicts to batched tensors"""
    batch_size = len(data_list)

    # Find max lengths
    max_src_len = max(len(d['src']) for d in data_list)
    max_clss_len = max(len(d['clss']) for d in data_list)

    # Initialize tensors
    src = torch.zeros(batch_size, max_src_len, dtype=torch.long)
    segs = torch.zeros(batch_size, max_src_len, dtype=torch.long)
    clss = torch.zeros(batch_size, max_clss_len, dtype=torch.long)
    mask_src = torch.zeros(batch_size, max_src_len, dtype=torch.float)
    mask_cls = torch.zeros(batch_size, max_clss_len, dtype=torch.float)

    # Fill tensors
    for i, data in enumerate(data_list):
        src_len = len(data['src'])
        clss_len = len(data['clss'])

        src[i, :src_len] = torch.tensor(data['src'])
        segs[i, :src_len] = torch.tensor(data['segs'])
        clss[i, :clss_len] = torch.tensor(data['clss'])
        mask_src[i, :src_len] = torch.tensor(data['mask_src'])
        mask_cls[i, :clss_len] = torch.tensor(data['mask_cls'])

    return src, segs, clss, mask_src, mask_cls



def extract_exports(py_path):
    """
    Parse a .py file and list all top-level class and function names
    that don't start with an underscore.
    """
    with open(py_path, 'r', encoding='utf-8') as f:
        node = ast.parse(f.read(), filename=py_path)
    exports = []
    for child in node.body:
        if isinstance(child, (ast.ClassDef, ast.FunctionDef)):
            if not child.name.startswith('_'):
                exports.append(child.name)
    return exports


def build_init(package_dir):
    """
    Generate __init__.py in package_dir, importing all public classes/functions
    from each module in that directory.
    """
    modules = []
    for fname in sorted(os.listdir(package_dir)):
        if not fname.endswith('.py') or fname == '__init__.py':
            continue
        mod_name = fname[:-3]
        exports = extract_exports(os.path.join(package_dir, fname))
        if exports:
            modules.append((mod_name, exports))
    init_lines = [
        f"# Auto-generated by load_presumm_model.py\n\n"
    ]
    all_names = []
    for mod, names in modules:
        names_list = ", ".join(names)
        init_lines.append(f"from .{mod} import {names_list}\n")
        all_names.extend(names)
    init_lines.append("\n__all__ = [\n")
    for name in all_names:
        init_lines.append(f"    '{name}',\n")
    init_lines.append("]\n")

    init_path = os.path.join(package_dir, '__init__.py')
    with open(init_path, 'w', encoding='utf-8') as f:
        f.writelines(init_lines)
    print(f"Generated {init_path} importing {len(all_names)} symbols.")


def clone_and_setup(repo_url, repo_dir):
    if not os.path.isdir(repo_dir):
        print(f"Cloning PreSumm from {repo_url} into {repo_dir}...")
        subprocess.run(['git', 'clone', repo_url, repo_dir], check=True)
    src_dir = os.path.join(repo_dir, 'src')
    # Rebuild __init__.py for models and others
    for subpkg in ['models', 'others']:
        build_init(os.path.join(src_dir, subpkg))
    # Add src to path for imports
    sys.path.insert(0, src_dir)
    print(f"Added {src_dir} to PYTHONPATH.")


def load_presumm_model(repo_dir, ckpt_path, device_str=None, repo_url=None):
    """
    Clone (if needed) and set up PreSumm, then load an XSum checkpoint
    into an ExtSummarizer model and return (model, tokenizer).
    """
    # Step 1: clone and generate __init__.py
    clone_and_setup(repo_url or 'https://github.com/nlpyang/PreSumm', repo_dir)

    # Step 2: import after path setup
    from models.model_builder import ExtSummarizer
    from models.adam import Adam
    from models.optimizers import Optimizer
    from others.utils import clean  # if needed elsewhere

    # Step 3: safe globals for checkpoint loading
    torch.serialization.add_safe_globals([
        Namespace,
        Adam,
        Optimizer,
        torch.optim.Adam
    ])

    # Step 4: build args and device
    args = Namespace(
        large=False,
        temp_dir='/tmp',
        finetune_bert=False,
        encoder='transformer',
        max_pos=512,
        ext_ff_size=2048,
        ext_heads=8,
        ext_dropout=0.1,
        ext_layers=2,
        ext_hidden_size=768,
        param_init=0.1,
        param_init_glorot=False
    )
    device = torch.device(device_str if device_str else ('cuda' if torch.cuda.is_available() else 'cpu'))

    # Step 5: load checkpoint
    print(f"Loading checkpoint from {ckpt_path} on {device}...")
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
    model_state = checkpoint.get('model', checkpoint)

    # Step 6: instantiate and load
    model = ExtSummarizer(args, device, None)
    model.load_state_dict(model_state, strict=False)
    model.to(device).eval()

    # Step 7: tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    print("Model and tokenizer loaded successfully.")
    return model, tokenizer


def main():
    parser = argparse.ArgumentParser(description='Load PreSumm ExtSummarizer model for XSum')
    parser.add_argument('--repo_dir', required=True,
                        help='Directory where PreSumm repo will be (or is) cloned')
    parser.add_argument('--ckpt_path', required=True,
                        help='Path to the .pt checkpoint file')
    parser.add_argument('--device', default=None,
                        help='torch device string (e.g. cpu or cuda:0)')
    parser.add_argument('--git_url', default=None,
                        help='PreSumm git URL (defaults to official)')
    args = parser.parse_args()

    model, tokenizer = load_presumm_model(
        repo_dir=args.repo_dir,
        ckpt_path=args.ckpt_path,
        device_str=args.device,
        repo_url=args.git_url
    )
    print(model)


if __name__ == '__main__':
    main()
